# ref: https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
---

# TODO: override name?
nameOverride: ""
fullnameOverride: ""

# NOTE: we'd like to create rules ourselves
defaultRules:
  create: false

alertmanager:
  enabled: true
  # TODO: enable PDB
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  config:
    global:
      resolve_timeout: 5m
    inhibit_rules:
      - source_matchers:
          - 'severity = critical'
        target_matchers:
          - 'severity =~ warning|info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'severity = warning'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'alertname = InfoInhibitor'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
    route:
      group_by: ['namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
      - receiver: 'null'
        matchers:
          - alertname =~ "InfoInhibitor|Watchdog"
    receivers:
    - name: 'null'
    templates:
    - '/etc/alertmanager/config/*.tmpl'

  templateFiles: {}
  #
  ## An example template:
  #   template_1.tmpl: |-
  #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}
  #
  #       {{ define "slack.myorg.text" }}
  #       {{- $root := . -}}
  #       {{ range .Alerts }}
  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
  #         *Cluster:* {{ template "cluster" $root }}
  #         *Description:* {{ .Annotations.description }}
  #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
  #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
  #         *Details:*
  #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`
  #           {{ end }}
  #       {{ end }}
  #       {{ end }}

  # TODO: better to create ingress ourselves?
  ingress:
    enabled: false

  # serviceMonitor:

  alertmanagerSpec:
    image:
      registry: quay.io
      repository: prometheus/alertmanager
      tag: v0.25.0
      sha: ""

    logFormat: logfmt
    logLevel: info

    replicas: 1

    # NOTE: retain silences, firing alerts
    retention: 120h
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: standard  # NOTE: kubernetes.io/gce-pd
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

    # TODO: need to add ingress host
    externalUrl:

    # TODO: configure scheduling?
    nodeSelector: {}
    resources:
      requests:
        memory: 256Mi
        cpu: 100m
      limits:
        memory: 256Mi
        # cpu: ""  # NOTE: nolimit

# DEPENDENCY: "6.51.*" https://github.com/grafana/helm-charts/tree/main/charts/grafana
grafana:
  enabled: true

  # TODO: fork dashboards
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: utc

  adminPassword: prom-operator

  ingress:
    enabled: false

  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "1"

    datasources:
      enabled: true
      defaultDatasourceEnabled: true
      isDefaultDatasource: true
      uid: prometheus
      # url: http://prometheus-stack-prometheus:9090/

      exemplarTraceIdDestinations: {}
        # datasourceUid: Jaeger
        # traceIdLabelName: trace_id
  additionalDataSources: []

  serviceMonitor:
    enabled: true


# -------------------------------------------------------
# ServiceMonitors

kubeApiServer:
  enabled: true

kubelet:
  enabled: true

# NOTE: unavailable in GKE
kubeControllerManager:
  enabled: false

coreDns:
  enabled: false

# TODO: looks like GKE actually using kube-dns instead of coreDns
kubeDns:
  enabled: true

kubeEtcd:
  enabled: false

kubeProxy:
  enabled: false

# -------------------------------------------------------
# Exporters

kubeStateMetrics:
  enabled: true

kube-state-metrics:
  prometheus:
    monitor:
      enabled: true

nodeExporter:
  enabled: true

prometheus-node-exporter:
  extraArgs:
    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
  prometheus:
    monitor:
      enabled: true
      metricRelabelings: []
      relabelings: []


prometheusOperator:
  enabled: true

  # NOTE: operator requirement
  admissionWebhooks:
    enabled: true
    patch:
      enabled: true
      image:
        registry: registry.k8s.io
        repository: ingress-nginx/kube-webhook-certgen
        tag: v20221220-controller-v1.5.1-58-g787ea74b6

  # NOTE: operator resources
  resources:
    requests:
      cpu: 100m
      memory: 200Mi
    limits:
      cpu: ""  # NOTE: nolimit
      memory: 200Mi

  # TODO: add scheduling?
  nodeSelector: {}

  image:
    registry: quay.io
    repository: prometheus-operator/prometheus-operator
    # if not set appVersion field from Chart.yaml is used
    tag: ""
    pullPolicy: IfNotPresent

  prometheusConfigReloader:
    image:
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      # if not set appVersion field from Chart.yaml is used
      tag: ""

    # resource config for prometheusConfigReloader
    resources:
      requests:
        cpu: ""  # NOTE: nolimit
        memory: 50Mi
      limits:
        cpu: 200m
        memory: 50Mi


# NOTE: finally prometheus instance (kind: Prometheus)
prometheus:
  enabled: true

  # TODO: add PDP?
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  ingress:
    enabled: false

  # serviceMonitor:

  ## Settings affecting prometheusSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
  prometheusSpec:
    disableCompaction: false
    apiserverConfig: {}
    additionalArgs: []
    enableAdminAPI: false

    exemplars: ""
    enableFeatures: []

    image:
      registry: quay.io
      repository: prometheus/prometheus
      tag: v2.42.0

    alertingEndpoints: []
    enableRemoteWriteReceiver: false

    externalUrl: ""

    # TODO: scheduling?
    nodeSelector: {}

    retention: 30d
    retentionSize: "85Gb"  # NOTE: 85% of PV

    walCompression: true
    paused: false

    replicas: 1
    shards: 1

    logLevel: info
    logFormat: logfmt

    # NOTE: we use single tenancy
    remoteRead: []
    additionalRemoteRead: []
    remoteWrite: []
    additionalRemoteWrite: []
    remoteWriteDashboards: false

    resources: {}
    requests:
      memory: 1Gi
      cpu: 250m
    limits:
      memory: 1Gi
      cpu: ""  # NOTE: no limit

    storageSpec:
     volumeClaimTemplate:
       spec:
         storageClassName: standart
         accessModes: ["ReadWriteOnce"]
         resources:
           requests:
             storage: 100Gi

    # NOTE: extra volumes, could be useful for scripts and data migration
    volumes: []
    volumeMounts: []

    # NOTE: for static jobs
    additionalScrapeConfigs: []

    securityContext:
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 2000

    priorityClassName: ""

    thanos: {}

    queryLogFile: false
    enforcedSampleLimit: false
    enforcedTargetLimit: false
    enforcedLabelLimit: false
    enforcedLabelNameLengthLimit: false
    enforcedLabelValueLengthLimit: false

cleanPrometheusOperatorObjectNames: false
